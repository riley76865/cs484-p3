{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 484 :: Data Mining :: George Mason University :: Spring 2023\n",
    "\n",
    "\n",
    "# Homework 3: Clustering&Association Rule Mining\n",
    "\n",
    "- **100 points [9% of your final grade]**\n",
    "- **Due Sunday, April 16 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* (1) implement your K-means model; and (2) implement the association rule mining process with the Apriori algorithm.\n",
    "\n",
    "- *Submission instructions:* for this homework, you only need to submit to Blackboard. Please name your submission **FirstName_Lastname_hw3.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw3.ipynb**. Your notebook should be fully executed so that we can see all outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Clustering (50 points)\n",
    "\n",
    "In this part, you will implement your own K-means algorithm to conduct clustering on handwritten digit images. In this homework, we will still use the handwritten digit image dataset we have already used in previous homework. However, since clustering is unsupervised learning, which means we do not use the label information anymore. So, here, we will only use the testing data stored in the \"test.txt\" file.\n",
    "\n",
    "First, let's load the data by excuting the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of testing feature matrix: shape (10000, 784)\n",
      "array of testing label matrix: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "test_features = test[:, 1:] # pixels for hand drawn image\n",
    "test_labels = test[:, 0]    # actual number corresponding to features\n",
    "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
    "print('array of testing label matrix: shape ' + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for you to implement your own K-means algorithm. First, please write your code to build your K-means model using the image data with **K = 10**, and **Euclidean distance**.\n",
    "\n",
    "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn**\n",
    "\n",
    "**Note: you need to decide when to stop the model training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 64504.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 71085.20it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 59724.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 74531.22it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 61540.75it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 66466.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 80185.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 76604.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 86456.61it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 58872.30it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 65460.41it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 83365.88it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 95596.02it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 95039.76it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 70277.54it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 63911.65it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 79742.96it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 66795.30it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 63162.09it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 71861.25it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 85405.27it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 86529.92it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 92899.77it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 84042.74it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 76308.91it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 60104.52it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 85058.53it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 58808.57it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 88345.40it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 86724.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 87032.84it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 87131.56it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 95676.48it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 70066.80it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 65936.46it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 75518.89it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 66428.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69532.37it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 74271.49it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68522.29it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 90611.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 80690.11it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 77856.70it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69909.03it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 56165.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 74430.31it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69469.95it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 79003.36it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 60143.13it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 89789.18it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 57632.27it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 72007.32it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 74606.65it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 78373.59it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 73110.60it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 53512.29it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 73511.54it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 73475.36it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68783.89it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 77594.56it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 70283.90it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 92675.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 62\n",
      "1684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from tqdm import tqdm\n",
    "K = 10 # number of clusters\n",
    "FEATURE_SIZE = 784\n",
    "num_features = len(test_features) # 10,000 samples\n",
    "#-------------------------------------------------------------------------------\n",
    "# select K points as initial centroids\n",
    "# repeat:\n",
    "#    form K clusters by assigning all points to the closest centroid\n",
    "#    recompute the centroid of each cluster\n",
    "# until: centroids dont change\n",
    "#-------------------------------------------------------------------------------\n",
    "centroid_indicies = np.random.randint(0,num_features,size=K,dtype=int)\n",
    "centroids = []\n",
    "for index in centroid_indicies:\n",
    "    centroids.append(test_features[index])\n",
    "centroids = np.asarray(centroids)\n",
    "#print(centroids)\n",
    "#-------------------------------------------------------------------------------\n",
    "def cluster(points,centroids):\n",
    "    current_centroids = centroids.copy()\n",
    "    previous_centroids = np.zeros(centroids.shape,dtype=int)\n",
    "    num_iterations = 0\n",
    "    clusters = np.full(num_features,-1) #initialize to number that isnt a label\n",
    "    # loop until centroids dont change\n",
    "    while np.array_equal(current_centroids,previous_centroids) == False:\n",
    "    #while True:\n",
    "        previous_centroids = current_centroids.copy()\n",
    "        num_iterations += 1\n",
    "        \n",
    "        # for every feature, calculate distance, find closest cluter and assign it\n",
    "        for i in tqdm(range(num_features)):\n",
    "            point = points[i]\n",
    "            # distance formula from hw1 sol\n",
    "            distances = np.sqrt(np.sum((point - current_centroids) ** 2, axis=1))\n",
    "            closest = int(np.argmin(distances,axis=0))\n",
    "            clusters[i] = closest\n",
    "        # re-center centroid after updating cluster assignments\n",
    "        for i in range(K):\n",
    "            current_centroids[i] = np.mean(points[clusters == i],axis=0)\n",
    "\n",
    "    print(f'Iterations: {num_iterations}')\n",
    "    #print(current_centroids)\n",
    "    return current_centroids,clusters\n",
    "#-------------------------------------------------------------------------------\n",
    "centroids_ret, clusters_ret = cluster(test_features,centroids)\n",
    "\n",
    "num_correct = 0\n",
    "for i in range(num_features):\n",
    "    if clusters_ret[i] == test_labels[i]:\n",
    "        num_correct += 1\n",
    "print(num_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to calculate the square root of Sum of Squared Error (SSE) of each cluster generated by your K-means algorithm. Then, print out the averaged SSE of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster [0] SSE: 4504671715.729\n",
      "Cluster [1] SSE: 3327420505.6\n",
      "Cluster [2] SSE: 1282729718.223\n",
      "Cluster [3] SSE: 5426438848.931\n",
      "Cluster [4] SSE: 3707375979.528\n",
      "Cluster [5] SSE: 4186860536.742\n",
      "Cluster [6] SSE: 3385982558.484\n",
      "Cluster [7] SSE: 3975840933.32\n",
      "Cluster [8] SSE: 2531176689.801\n",
      "Cluster [9] SSE: 4169276654.149\n",
      "Average SSE: 4655328.334\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# create K element array to hold summed differences\n",
    "cluster_sums = np.zeros((K,FEATURE_SIZE))\n",
    "\n",
    "for index in clusters_ret:\n",
    "    \n",
    "    index = int(index)\n",
    "    point = test_features[index]\n",
    "    centroid = centroids_ret[index]\n",
    "    difference = ((point - centroid)**2)\n",
    "    cluster_sums[index] += difference\n",
    "    #print(difference)\n",
    "\n",
    "for i in range(K):\n",
    "    print(f'Cluster [{i}] SSE: {np.round(np.sum(cluster_sums[i]),3)}')\n",
    "print(f'Average SSE: {np.round(np.average(cluster_sums),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, please have a look on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure, and use this function to print out the homogeneity, completeness, and v-measure of your K-means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.522\n",
      "Completeness: 0.537\n",
      "V-Measure: 0.529\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import sklearn.metrics\n",
    "H,C,V = sklearn.metrics.homogeneity_completeness_v_measure(test_labels,clusters_ret) #gt,pred,beta\n",
    "print(f'Homogeneity: {np.round(H,3)}')\n",
    "print(f'Completeness: {np.round(C,3)}')\n",
    "print(f'V-Measure: {np.round(V,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Association Rule Mining (50 points)\n",
    "\n",
    "In this part, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n"
     ]
    }
   ],
   "source": [
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id rated the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been rated by a single user as a transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. \n",
    "\n",
    "**Note: Do not copy-paste any existing code.**\n",
    "\n",
    "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
    "\n",
    "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total users: 11743\n",
      "number of total movies: 11743\n",
      "number of unique users: 494\n",
      "number of unique movies: 494\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "from itertools import combinations\n",
    "#include all the helpful print statements\n",
    "# when you run this block, we want to see all of your intermediate steps\n",
    "# you can save the rules you discover for printing in the following cells (this will help us grade by keeping these separate)\n",
    "K = 2 # from lecture slides\n",
    "# support - % of transactions that contain an interest\n",
    "MIN_SUPPORT = 0.2\n",
    "# confidence - how often items in Y appear in transactions that contain X\n",
    "MIN_CONFIDENCE = 0.8\n",
    "\n",
    "#MAX_ITERATIONS = 400 #TODO: change this\n",
    "\n",
    "# combine all movies seen by one userID into a transaction\n",
    "\n",
    "# split dataset into users and movies\n",
    "total_users, total_movies = np.split(user_movie_data,2,axis=1)\n",
    "# get unique elements from each array\n",
    "unique_movies = np.unique(total_movies)\n",
    "unique_users = np.unique(total_users)\n",
    "\n",
    "num_unique_users = len(unique_users)\n",
    "num_unique_movies = len(unique_users)\n",
    "num_total_users = len(total_users)\n",
    "num_total_movies = len(total_movies)\n",
    "\n",
    "print(f'number of total users: {num_total_users}')\n",
    "print(f'number of total movies: {num_total_movies}')\n",
    "print(f'number of unique users: {num_unique_users}')\n",
    "print(f'number of unique movies: {num_unique_movies}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11743/11743 [00:00<00:00, 1542263.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# create transactions from matrix data\n",
    "# add to dictionary\n",
    "# idk this seemed like a good way to store the transactions based on userID\n",
    "def generate_transactions(dataset):\n",
    "    transactions = {}\n",
    "    num_entries,width = dataset.shape\n",
    "    # for every user, see if theyre in transactions, if not, add\n",
    "    for i in tqdm(range(num_entries)): # looping this way to be able to use TQDM for progress\n",
    "        row = dataset[i]\n",
    "        user = row[0]\n",
    "        movie = row[1]\n",
    "        # add every unique user to dict\n",
    "        if user not in transactions.keys():\n",
    "            transactions[user] = []\n",
    "        if movie not in transactions[user]:\n",
    "            transactions[user].append(movie)\n",
    "    return transactions\n",
    "#print(transactions)\n",
    "# get transactions\n",
    "transactions = generate_transactions(user_movie_data)\n",
    "# convert transactions into array\n",
    "transaction_list = list(transactions.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "number of transactions: 494\n"
     ]
    }
   ],
   "source": [
    "# make sure number of keys in transactions is the same as the number of unique users\n",
    "print(len(transaction_list)==num_unique_users)\n",
    "num_transactions = num_unique_users\n",
    "print(f'number of transactions: {len(transaction_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 494/494 [00:00<00:00, 276892.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of 1 elemenent frequent set: 21\n",
      "frequent_set: [480.0, 1221.0, 1270.0, 1265.0, 1197.0, 260.0, 1961.0, 527.0, 2028.0, 2987.0, 858.0, 1584.0, 2710.0, 1094.0, 1252.0, 587.0, 1387.0, 3255.0, 2706.0, 377.0, 1127.0]\n",
      "K: 2\n",
      "Generating 1 element combos\n",
      "Number of candidate combos: 210\n",
      "Added: (1270.0, 1265.0)\n",
      "Added: (1270.0, 1197.0)\n",
      "Added: (1270.0, 260.0)\n",
      "Added: (1270.0, 527.0)\n",
      "Added: (1270.0, 2028.0)\n",
      "Added: (1265.0, 1197.0)\n",
      "Added: (1265.0, 260.0)\n",
      "Added: (1265.0, 527.0)\n",
      "Added: (1265.0, 2028.0)\n",
      "Added: (1197.0, 260.0)\n",
      "Added: (1197.0, 527.0)\n",
      "Added: (1197.0, 2028.0)\n",
      "Added: (260.0, 527.0)\n",
      "Added: (260.0, 2028.0)\n",
      "Added: (527.0, 2028.0)\n",
      "Added: (480.0, 2028.0)\n",
      "Added: (480.0, 1270.0)\n",
      "Added: (480.0, 1265.0)\n",
      "Added: (480.0, 1197.0)\n",
      "Added: (480.0, 260.0)\n",
      "Added: (480.0, 527.0)\n",
      "Added: (480.0, 858.0)\n",
      "Added: (1270.0, 2987.0)\n",
      "Added: (1270.0, 858.0)\n",
      "Added: (1197.0, 2987.0)\n",
      "Added: (1197.0, 858.0)\n",
      "Added: (260.0, 2987.0)\n",
      "Added: (260.0, 858.0)\n",
      "Added: (527.0, 858.0)\n",
      "Added: (1221.0, 260.0)\n",
      "Added: (1221.0, 858.0)\n",
      "Added: (2028.0, 858.0)\n",
      "Added: (480.0, 377.0)\n",
      "Added: (260.0, 1387.0)\n",
      "Added: (260.0, 377.0)\n",
      "Added: (260.0, 1127.0)\n",
      "frequent_set: [480.0, 1221.0, 1270.0, 1265.0, 1197.0, 260.0, 1961.0, 527.0, 2028.0, 2987.0, 858.0, 1584.0, 2710.0, 1094.0, 1252.0, 587.0, 1387.0, 3255.0, 2706.0, 377.0, 1127.0, (1270.0, 1265.0), (1270.0, 1197.0), (1270.0, 260.0), (1270.0, 527.0), (1270.0, 2028.0), (1265.0, 1197.0), (1265.0, 260.0), (1265.0, 527.0), (1265.0, 2028.0), (1197.0, 260.0), (1197.0, 527.0), (1197.0, 2028.0), (260.0, 527.0), (260.0, 2028.0), (527.0, 2028.0), (480.0, 2028.0), (480.0, 1270.0), (480.0, 1265.0), (480.0, 1197.0), (480.0, 260.0), (480.0, 527.0), (480.0, 858.0), (1270.0, 2987.0), (1270.0, 858.0), (1197.0, 2987.0), (1197.0, 858.0), (260.0, 2987.0), (260.0, 858.0), (527.0, 858.0), (1221.0, 260.0), (1221.0, 858.0), (2028.0, 858.0), (480.0, 377.0), (260.0, 1387.0), (260.0, 377.0), (260.0, 1127.0)]\n",
      "K: 3\n",
      "Generating 2 element combos\n",
      "Number of candidate combos: 1330\n",
      "Added: (1270.0, 1265.0, 1197.0)\n",
      "Added: (1270.0, 1265.0, 260.0)\n",
      "Added: (1270.0, 1197.0, 260.0)\n",
      "Added: (1270.0, 260.0, 527.0)\n",
      "Added: (1270.0, 260.0, 2028.0)\n",
      "Added: (1265.0, 1197.0, 260.0)\n",
      "Added: (1197.0, 260.0, 2028.0)\n",
      "Added: (260.0, 527.0, 2028.0)\n",
      "Added: (480.0, 1270.0, 260.0)\n",
      "Added: (480.0, 1197.0, 260.0)\n",
      "Added: (1221.0, 260.0, 858.0)\n",
      "Added: (480.0, 260.0, 2028.0)\n",
      "frequent_set: [480.0, 1221.0, 1270.0, 1265.0, 1197.0, 260.0, 1961.0, 527.0, 2028.0, 2987.0, 858.0, 1584.0, 2710.0, 1094.0, 1252.0, 587.0, 1387.0, 3255.0, 2706.0, 377.0, 1127.0, (1270.0, 1265.0), (1270.0, 1197.0), (1270.0, 260.0), (1270.0, 527.0), (1270.0, 2028.0), (1265.0, 1197.0), (1265.0, 260.0), (1265.0, 527.0), (1265.0, 2028.0), (1197.0, 260.0), (1197.0, 527.0), (1197.0, 2028.0), (260.0, 527.0), (260.0, 2028.0), (527.0, 2028.0), (480.0, 2028.0), (480.0, 1270.0), (480.0, 1265.0), (480.0, 1197.0), (480.0, 260.0), (480.0, 527.0), (480.0, 858.0), (1270.0, 2987.0), (1270.0, 858.0), (1197.0, 2987.0), (1197.0, 858.0), (260.0, 2987.0), (260.0, 858.0), (527.0, 858.0), (1221.0, 260.0), (1221.0, 858.0), (2028.0, 858.0), (480.0, 377.0), (260.0, 1387.0), (260.0, 377.0), (260.0, 1127.0), (1270.0, 1265.0, 1197.0), (1270.0, 1265.0, 260.0), (1270.0, 1197.0, 260.0), (1270.0, 260.0, 527.0), (1270.0, 260.0, 2028.0), (1265.0, 1197.0, 260.0), (1197.0, 260.0, 2028.0), (260.0, 527.0, 2028.0), (480.0, 1270.0, 260.0), (480.0, 1197.0, 260.0), (1221.0, 260.0, 858.0), (480.0, 260.0, 2028.0)]\n",
      "K: 4\n",
      "Generating 3 element combos\n",
      "Number of candidate combos: 5985\n",
      "Breaking!\n",
      "Length of frequent set: 69\n",
      "[480.0, 1221.0, 1270.0, 1265.0, 1197.0, 260.0, 1961.0, 527.0, 2028.0, 2987.0, 858.0, 1584.0, 2710.0, 1094.0, 1252.0, 587.0, 1387.0, 3255.0, 2706.0, 377.0, 1127.0, (1270.0, 1265.0), (1270.0, 1197.0), (1270.0, 260.0), (1270.0, 527.0), (1270.0, 2028.0), (1265.0, 1197.0), (1265.0, 260.0), (1265.0, 527.0), (1265.0, 2028.0), (1197.0, 260.0), (1197.0, 527.0), (1197.0, 2028.0), (260.0, 527.0), (260.0, 2028.0), (527.0, 2028.0), (480.0, 2028.0), (480.0, 1270.0), (480.0, 1265.0), (480.0, 1197.0), (480.0, 260.0), (480.0, 527.0), (480.0, 858.0), (1270.0, 2987.0), (1270.0, 858.0), (1197.0, 2987.0), (1197.0, 858.0), (260.0, 2987.0), (260.0, 858.0), (527.0, 858.0), (1221.0, 260.0), (1221.0, 858.0), (2028.0, 858.0), (480.0, 377.0), (260.0, 1387.0), (260.0, 377.0), (260.0, 1127.0), (1270.0, 1265.0, 1197.0), (1270.0, 1265.0, 260.0), (1270.0, 1197.0, 260.0), (1270.0, 260.0, 527.0), (1270.0, 260.0, 2028.0), (1265.0, 1197.0, 260.0), (1197.0, 260.0, 2028.0), (260.0, 527.0, 2028.0), (480.0, 1270.0, 260.0), (480.0, 1197.0, 260.0), (1221.0, 260.0, 858.0), (480.0, 260.0, 2028.0)]\n"
     ]
    }
   ],
   "source": [
    "# [x] -> [Y] format\n",
    "K = 2\n",
    "#-------------------------------------------------------------------------------\n",
    "# method for computing support for list of movies\n",
    "def calculate_support(_dict,movie):\n",
    "    return _dict.get(movie)/(len(transaction_list))\n",
    "#-------------------------------------------------------------------------------\n",
    "# method for computing confidence for a list of movies\n",
    "# = support(X)/support(Y)\n",
    "#TODO: implement confidence\n",
    "def confidence(X,Y,_dict):\n",
    "    return calculate_support(_dict,X.union(Y))/calculate_support(_dict,X)\n",
    "#-------------------------------------------------------------------------------\n",
    "def generate_initial_frequent_set(transactions):\n",
    "    movie_count = {}\n",
    "    for i in tqdm(range(len(transactions))):\n",
    "        transaction = transactions[i]\n",
    "        # inc count for every movie, create K,V pair if DNE\n",
    "        for movie in transaction:\n",
    "            if movie not in movie_count.keys():\n",
    "                movie_count[movie] = 0\n",
    "            movie_count[movie] += 1\n",
    "    # create 1 element frequent set\n",
    "    frequent_set = []\n",
    "    for movie in movie_count.keys():\n",
    "        support = calculate_support(movie_count,movie)\n",
    "        if support >= MIN_SUPPORT:\n",
    "            frequent_set.append(movie)\n",
    "    return frequent_set\n",
    "#-------------------------------------------------------------------------------\n",
    "frequent_set = generate_initial_frequent_set(transaction_list)\n",
    "len_frequent_itemset_1 = len(frequent_set)\n",
    "print(f'size of 1 elemenent frequent set: {len_frequent_itemset_1}')\n",
    "#-------------------------------------------------------------------------------\n",
    "def candidate_sets_combinations(frequent_set,K):\n",
    "    stop = False\n",
    "    # loop until no new candidates are added to frequent set\n",
    "    while stop == False:\n",
    "        print(f'frequent_set: {frequent_set}')\n",
    "        print(f'K: {K}')\n",
    "        print(f'Generating {K-1} element combos')\n",
    "        num_added = 0\n",
    "        candidate_count = {}\n",
    "        # generate K length combinations of frequent set\n",
    "        candidate_set = combinations(frequent_set,K)\n",
    "        candidate_set = list(candidate_set) # convert to list of tuples\n",
    "        # count occurances of candidates for support\n",
    "        for transaction in transaction_list:\n",
    "            for candidate in candidate_set:\n",
    "                #print(f'candidate: {candidate}')\n",
    "                #print(f'transaction: {transaction}')\n",
    "                # if candidate is in transaction, inc count\n",
    "                if set(candidate).issubset(set(transaction)):\n",
    "                    #print('in here!')\n",
    "                    if candidate not in candidate_count.keys():\n",
    "                        candidate_count[candidate] = 0\n",
    "                    #if candidate in candidate_count.keys():\n",
    "                    candidate_count[candidate] += 1\n",
    "\n",
    "        # add supporting candidates to frequent set\n",
    "        print(f'Number of candidate combos: {len(candidate_count.keys())}')\n",
    "        for candidate in candidate_count.keys():\n",
    "            support = calculate_support(candidate_count,candidate)\n",
    "            #print(support)\n",
    "            if support >= MIN_SUPPORT:\n",
    "                print(f'Added: {candidate}')\n",
    "                num_added += 1\n",
    "                frequent_set.append(candidate)\n",
    "        K += 1\n",
    "        if num_added == 0:\n",
    "            stop = True\n",
    "            print('Breaking!')\n",
    "            break\n",
    "    return frequent_set\n",
    "#-------------------------------------------------------------------------------\n",
    "# generate candidate sets\n",
    "# OLD!!!!!\n",
    "def generate_candidate_sets(frequent_set):\n",
    "    return\n",
    "#-------------------------------------------------------------------------------\n",
    "# OLD\n",
    "#frequent_set = generate_candidate_sets(frequent_set)\n",
    "#print(f'len frequent set: {len(frequent_set)}')\n",
    "#print(frequent_set)\n",
    "\n",
    "frequent_set = candidate_sets_combinations(frequent_set,K)\n",
    "print(f'Length of frequent set: {len(frequent_set)}')\n",
    "print(frequent_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480.0\n",
      "1221.0\n",
      "1270.0\n",
      "1265.0\n",
      "1197.0\n",
      "260.0\n",
      "1961.0\n",
      "527.0\n",
      "2028.0\n",
      "2987.0\n",
      "858.0\n",
      "1584.0\n",
      "2710.0\n",
      "1094.0\n",
      "1252.0\n",
      "587.0\n",
      "1387.0\n",
      "3255.0\n",
      "2706.0\n",
      "377.0\n",
      "1127.0\n",
      "(1270.0, 1265.0)\n",
      "(1270.0, 1197.0)\n",
      "(1270.0, 260.0)\n",
      "(1270.0, 527.0)\n",
      "(1270.0, 2028.0)\n",
      "(1265.0, 1197.0)\n",
      "(1265.0, 260.0)\n",
      "(1265.0, 527.0)\n",
      "(1265.0, 2028.0)\n",
      "(1197.0, 260.0)\n",
      "(1197.0, 527.0)\n",
      "(1197.0, 2028.0)\n",
      "(260.0, 527.0)\n",
      "(260.0, 2028.0)\n",
      "(527.0, 2028.0)\n",
      "(480.0, 2028.0)\n",
      "(480.0, 1270.0)\n",
      "(480.0, 1265.0)\n",
      "(480.0, 1197.0)\n",
      "(480.0, 260.0)\n",
      "(480.0, 527.0)\n",
      "(480.0, 858.0)\n",
      "(1270.0, 2987.0)\n",
      "(1270.0, 858.0)\n",
      "(1197.0, 2987.0)\n",
      "(1197.0, 858.0)\n",
      "(260.0, 2987.0)\n",
      "(260.0, 858.0)\n",
      "(527.0, 858.0)\n",
      "(1221.0, 260.0)\n",
      "(1221.0, 858.0)\n",
      "(2028.0, 858.0)\n",
      "(480.0, 377.0)\n",
      "(260.0, 1387.0)\n",
      "(260.0, 377.0)\n",
      "(260.0, 1127.0)\n",
      "(1270.0, 1265.0, 1197.0)\n",
      "(1270.0, 1265.0, 260.0)\n",
      "(1270.0, 1197.0, 260.0)\n",
      "(1270.0, 260.0, 527.0)\n",
      "(1270.0, 260.0, 2028.0)\n",
      "(1265.0, 1197.0, 260.0)\n",
      "(1197.0, 260.0, 2028.0)\n",
      "(260.0, 527.0, 2028.0)\n",
      "(480.0, 1270.0, 260.0)\n",
      "(480.0, 1197.0, 260.0)\n",
      "(1221.0, 260.0, 858.0)\n",
      "(480.0, 260.0, 2028.0)\n",
      "combos: [[480.0, 260.0], [480.0, 2028.0], [260.0, 2028.0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate rules from frequent set\n",
    "frequent_set_copy = frequent_set.copy()\n",
    "def get_rules(frequent_set):\n",
    "    association_rules = []\n",
    "    for _set in frequent_set:\n",
    "        combos = []\n",
    "        print(_set)\n",
    "        # if single element in set\n",
    "        if isinstance(_set,np.floating):\n",
    "            combos.append(_set)\n",
    "        else:\n",
    "            # if two element set\n",
    "            if len(_set) == 2:\n",
    "                combos.append([_set[0],_set[1]])\n",
    "            # if > 2 element set\n",
    "            else:\n",
    "                for i in range(len(_set)):\n",
    "                    for j in range(i+1,len(_set)):\n",
    "                        combos.append([_set[i],_set[j]])\n",
    "    # for each rule generated, calculate confidence and if >= MIN_CONFIDNECE, add to assoc_rules\n",
    "    \n",
    "    print(f'combos: {combos}')\n",
    "get_rules(frequent_set_copy)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print your final association rules in the following format:\n",
    "\n",
    "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
    "\n",
    "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
    "\n",
    "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
    "\n",
    "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as use pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/riley/Library/CloudStorage/GoogleDrive-riley76865@gmail.com/My Drive/School/Senior Year/Spring 2023/CS484/hw3/hw3_backup.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m frequent_set_copy \u001b[39m=\u001b[39m frequent_set\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#print(movie_names.iloc[4][1])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# loop over every rule in rules\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m _:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# loop over every element in each rule\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m _:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/riley/Library/CloudStorage/GoogleDrive-riley76865%40gmail.com/My%20Drive/School/Senior%20Year/Spring%202023/CS484/hw3/hw3_backup.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         movie \u001b[39m=\u001b[39m movie_names\u001b[39m.\u001b[39miloc[\u001b[39mint\u001b[39m(movie)][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Write your code to print out the rules\n",
    "movie_names = pd.read_csv('movies.csv')\n",
    "movie_names.head()\n",
    "\n",
    "frequent_set_copy = frequent_set.copy()\n",
    "\n",
    "#print(movie_names.iloc[4][1])\n",
    "# loop over every rule in rules\n",
    "for _ in _:\n",
    "    # loop over every element in each rule\n",
    "    for _ in _:\n",
    "        movie = movie_names.iloc[int(movie)][1]\n",
    "print(frequent_set_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
